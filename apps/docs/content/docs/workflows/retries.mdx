---
title: Retries & Error Handling
description: Configure retry strategies and handle failures gracefully
---

import { Callout } from 'fumadocs-ui/components/callout';

# Retries & Error Handling

Every `step.do()` has automatic retry support built-in. Network hiccup? API rate limit? Transient database error? Ablauf will retry the step automatically using Durable Object alarms (non-blocking, of course).

## Default Behavior

By default, every step gets **3 retry attempts** with **1 second delay** and **exponential backoff**:

```ts
// Uses defaults: { limit: 3, delay: "1s", backoff: "exponential" }
const data = await step.do('fetch-data', async () => {
	const res = await fetch('https://api.example.com/data');
	if (!res.ok) throw new Error(`HTTP ${res.status}`);
	return res.json();
});
```

If the step fails all 3 times, it throws `StepRetryExhaustedError` and the workflow stops.

## Per-Step Overrides

Need more retries for a critical operation? Override the defaults:

```ts
const data = await step.do(
	'critical-operation',
	async () => {
		// This step gets 10 attempts with 5s delay
		return await somethingFragile();
	},
	{
		retries: {
			limit: 10,
			delay: '5s',
			backoff: 'exponential',
		},
	},
);
```

## Workflow-Level Defaults

Set default retry config for all steps in a workflow:

```ts
import { defineWorkflow } from '@der-ablauf/workflows';
import { z } from 'zod';

const MyWorkflow = defineWorkflow({
	type: 'my-workflow',
	input: z.object({
		/* ... */
	}),
	defaults: {
		retries: { limit: 5, delay: '2s', backoff: 'linear' },
	},
	run: async (step, payload) => {
		// All steps in this workflow default to 5 attempts with linear backoff
		await step.do('step-1', async () => {
			/* ... */
		});
		await step.do('step-2', async () => {
			/* ... */
		});
	},
});
```

Per-step overrides still work â€” they take precedence over workflow defaults.

## Backoff Strategies

Ablauf supports three backoff strategies:

| Strategy        | Formula                 | Example (1s base delay) |
| --------------- | ----------------------- | ----------------------- |
| `"fixed"`       | `delay`                 | 1s, 1s, 1s, 1s          |
| `"linear"`      | `delay * attempt`       | 1s, 2s, 3s, 4s          |
| `"exponential"` | `delay * 2^(attempt-1)` | 1s, 2s, 4s, 8s          |

Exponential backoff is usually the right choice â€” it gives temporary issues time to resolve without hammering a struggling service.

Retry delays use [duration strings](/docs/server/api-reference#duration-strings) like `"500ms"`, `"1s"`, `"30s"`, `"5m"`, or `"1h"`.

## When Retries Are Exhausted

When all retry attempts fail, the step throws `StepRetryExhaustedError`. This propagates to the workflow's `run()` function, marking the workflow as errored.

```ts
try {
	await step.do('flaky-operation', async () => {
		// This might fail...
	});
} catch (err) {
	if (err instanceof StepRetryExhaustedError) {
		// Log the failure, send an alert, etc.
		console.error(`Step failed after ${err.attempts} attempts`);
	}
	throw err;
}
```

<Callout type="info">
	For the complete list of error classes and their HTTP status codes, see the [API Reference](/docs/server/api-reference#error-classes).
</Callout>

## Skipping Retries with NonRetriableError

Sometimes retrying is pointless â€” the error is permanent and will never succeed. For these cases, throw `NonRetriableError` inside your step function to immediately fail the step without retrying:

```ts
import { defineWorkflow, NonRetriableError } from '@der-ablauf/workflows';

const order = defineWorkflow({
	type: 'process-order',
	input: z.object({ userId: z.string() }),
	run: async (step, payload) => {
		const user = await step.do('validate-user', async () => {
			const user = await getUser(payload.userId);
			if (user.banned) {
				throw new NonRetriableError('User is banned');
			}
			return user;
		});
		// ...
	},
});
```

When `NonRetriableError` is thrown:

1. The step is immediately marked as `failed` â€” no retries are attempted, regardless of the retry configuration
2. The error is recorded in the step's retry history (visible in the dashboard)
3. The workflow transitions to `errored`

<Callout type="info">
	`NonRetriableError` extends plain `Error`, not `WorkflowError`. It's designed to be simple for user code â€” no error codes or HTTP statuses needed.
</Callout>

### When to Use NonRetriableError

Use it for errors where retrying would be wasteful:

- **Business rule violations** â€” user is banned, account is suspended
- **Authorization failures** â€” invalid API key, insufficient permissions
- **Invalid data** â€” malformed input discovered mid-step
- **Resource gone** â€” the thing you need no longer exists

## Crash & OOM Recovery

Cloudflare Durable Objects run in isolates with a **128 MB memory limit**. If a step exceeds this limit (or the isolate crashes for any reason), the entire isolate is killed â€” your step's try/catch never executes, and the error is never recorded.

Ablauf handles this automatically using **write-ahead step tracking**. Before executing your step function, Ablauf persists the step as `"running"` in SQLite with an incremented attempt counter. If the isolate dies mid-execution:

1. The step remains in `"running"` state in durable storage (SQLite survives isolate resets)
2. A **safety alarm** (set before replay started) fires and triggers the alarm handler
3. The alarm handler replays the workflow, detecting the orphaned `"running"` step
4. The crash is recorded in the step's retry history
5. Normal retry logic kicks in â€” backoff delay, then re-execution
6. If retries are exhausted, the step fails permanently with `StepRetryExhaustedError`

This means OOM crashes are handled identically to normal step failures â€” no infinite loops, no zombie workflows. Ablauf sets a safety alarm before every replay so there's always a trigger for crash recovery, whether the OOM happened during initial execution, a resume, or an alarm-driven retry.

```
Attempt 1: step.do() â†’ write-ahead (running, attempts=1) â†’ fn() â†’ OOM ðŸ’¥
  â†“ isolate killed, safety alarm fires
Recovery: alarm() â†’ replay() â†’ detects status="running" â†’ schedule retry with backoff
  â†“ retry alarm fires
Attempt 2: step.do() â†’ write-ahead (running, attempts=2) â†’ fn() â†’ success âœ“
```

<Callout type="warn">
	If a step deterministically exceeds 128 MB, retries won't help â€” it will fail on every attempt and
	eventually exhaust retries. Move memory-heavy work to a separate Worker via
	[service binding RPC](https://developers.cloudflare.com/workers/runtime-apis/bindings/service-bindings/rpc/)
	so the DO isolate stays safe. The separate Worker has its own 128 MB and its crash won't kill your workflow's state.
</Callout>

### What Survives an Isolate Crash

| Data | Survives? | Why |
|------|-----------|-----|
| Completed step results | Yes | Persisted in SQLite before the crash |
| Workflow metadata & payload | Yes | Written to durable storage on creation |
| In-flight step attempt counter | Yes | Write-ahead persists before `fn()` runs |
| The step function's return value | No | Isolate died before it could be saved |
| JavaScript variables in memory | No | Isolate memory is wiped |

## Best Practices

**Set realistic retry limits.** If an API is down, 100 retries won't help. Use retries for transient issues, not systemic failures.

**Use exponential backoff for external services.** Linear or fixed backoff can overwhelm a struggling service.

**Don't retry non-idempotent operations blindly.** If retrying a step could cause duplicate charges, emails, or data corruption, add idempotency checks inside the step.

```ts
await step.do(
	'send-email',
	async () => {
		// Check if email was already sent before retrying
		const alreadySent = await checkEmailLog(userId);
		if (alreadySent) return;

		await sendEmail(userId, 'Welcome!');
	},
	{
		retries: { limit: 5 },
	},
);
```

**Offload memory-heavy steps.** If a step does inference, image processing, or anything that might exceed 128 MB, call it via `fetch()` or service binding RPC so it runs in a separate isolate. Your DO stays safe and can retry if the external call fails.

```ts
await step.do('run-inference', async () => {
	// Runs in a separate Worker's isolate â€” OOM here won't kill the DO
	const result = await env.INFERENCE_WORKER.runModel(payload);
	return result;
}, { retries: { limit: 5, delay: '10s', backoff: 'exponential' } });
```
